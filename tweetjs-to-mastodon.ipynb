{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fediporter\n",
    "\n",
    "Jupyter notebook to migrate content like Tweets oder Mastodon posts to Mastodon.\n",
    "\n",
    "Only works with instances that are patched to allow backdated posts through the API. More info: https://github.com/lucahammer/fediporter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T10:05:14.110272Z",
     "start_time": "2022-12-23T10:05:14.107618Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime\n",
    "import json\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.json', 'r') as f:\n",
    "    config = json.loads(f.read())\n",
    "\n",
    "API_BASE_URL = config['mastodon_url']\n",
    "MASTODON_BEARER = config['mastodon_bearer']\n",
    "TWITTER_BEARER = config['twitter_bearer']\n",
    "\n",
    "DATA_DIR = config['data_dir'] # Unzipped twitter data export\n",
    "media_dir_backup = config['media_dir_backup'] # media folder of twitter data export\n",
    "media_dir = config['media_dir'] # media folder of https://github.com/timhutton/twitter-archive-parser\n",
    "\n",
    "# Test Mastodon bearer token\n",
    "url = f\"{API_BASE_URL}/api/v1/apps/verify_credentials\"\n",
    "r = requests.get(url, headers=HEADERS)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T11:57:44.037582Z",
     "start_time": "2022-12-23T11:57:43.929260Z"
    }
   },
   "outputs": [],
   "source": [
    "def post_status(data):\n",
    "    HEADERS = {'Authorization': f'Bearer {MASTODON_BEARER}'}\n",
    "    url = f\"{API_BASE_URL}/api/v1/statuses\"\n",
    "    r = requests.post(url,\n",
    "                      data=data,\n",
    "                      headers=HEADERS)\n",
    "    return r.json()\n",
    "\n",
    "\n",
    "def load_tweets():\n",
    "    with open(data_dir+\"tweets.js\", 'r', encoding='utf8') as f:\n",
    "        raw = f.read()\n",
    "    raw = raw.replace(\"window.YTD.tweets.part0 = \", \"\")\n",
    "    tweets = json.loads(raw)\n",
    "    tweets = [tweet['tweet'] for tweet in tweets]\n",
    "    tweets = sorted(tweets, key=lambda d: int(d['id']))\n",
    "    return tweets\n",
    "\n",
    "\n",
    "def to_timestamp(created_at):\n",
    "    timestamp = datetime.datetime.strptime(\n",
    "        created_at, '%a %b %d %X %z %Y').isoformat(timespec='seconds')\n",
    "    return timestamp\n",
    "\n",
    "\n",
    "def replace_urls(tweet):\n",
    "    if 'full_text' in tweet:\n",
    "        text = tweet['full_text']\n",
    "    else:\n",
    "        text = tweet['text']\n",
    "    if 'entities' in tweet and 'urls' in tweet['entities']:\n",
    "        for url in tweet['entities']['urls']:\n",
    "            text = text.replace(url['url'], url['expanded_url'])\n",
    "    return (text)\n",
    "\n",
    "\n",
    "def replace_usernames(text):\n",
    "    text = re.sub(r\"(\\B\\@[A-Za-z0-9_]{1,15})(\\:)?\", r\"\\1@twitter.com\\2\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def tweet_to_toot(tweet):\n",
    "    toot = {\n",
    "        'status': replace_usernames(replace_urls(tweet)),\n",
    "        'visibility': 'public',\n",
    "        'created_at': to_timestamp(tweet['created_at']),\n",
    "        'language': tweet['lang']\n",
    "    }\n",
    "    return toot\n",
    "\n",
    "\n",
    "def retrieve_alt_texts(tweet_ids):\n",
    "    # get alt text for specific IDs from Twitter API\n",
    "    twitter_url = \"https://api.twitter.com/2/tweets\"\n",
    "    twitter_heeaders = {\"Authorization\": f\"Bearer {TWITTER_BEARER}\"}\n",
    "    twitter_params = {'ids': ','.join(tweet_ids),\n",
    "                      'tweet.fields': 'text,attachments,entities',\n",
    "                      'expansions': 'attachments.media_keys',\n",
    "                      'media.fields': 'alt_text'\n",
    "                      }\n",
    "    resp = requests.get(\n",
    "        twitter_url, headers=twitter_heeaders, params=twitter_params)\n",
    "    resp_json = resp.json()\n",
    "\n",
    "    for media in resp_json['includes']['media']:\n",
    "        if 'alt_text' in media:\n",
    "            alt_texts[media['media_key']] = media['alt_text']\n",
    "\n",
    "\n",
    "def add_alt_texts(tweets):\n",
    "    # looks for Tweets with media and asks Twitter API for alt texts\n",
    "    # adds those alt texts to the dict alt_texts\n",
    "    tweets_with_media = [tweet for tweet in tweets[50100:]\n",
    "                         if 'media' in tweet['entities']]\n",
    "    print(f'Found {len(tweets_with_media)} Tweets with media attachements.')\n",
    "    tweet_ids = [str(tweets_with_media['id'])\n",
    "                 for tweets_with_media in tweets_with_media]\n",
    "    batches = [tweet_ids[idx:idx+100] for idx in range(0, len(tweet_ids), 100)]\n",
    "\n",
    "    for batch in tqdm(batches):\n",
    "        retrieve_alt_texts(batch)\n",
    "\n",
    "    print(f'Found {len(alt_texts)} alt texts.')\n",
    "\n",
    "\n",
    "def retrieve_rt_texts(tweet_ids):\n",
    "    # get full text of Retweets for specific IDs from Twitter API\n",
    "    twitter_url = \"https://api.twitter.com/2/tweets\"\n",
    "    twitter_heeaders = {\"Authorization\": f\"Bearer {TWITTER_BEARER}\"}\n",
    "    twitter_params = {'ids': ','.join(tweet_ids),\n",
    "                      'tweet.fields': 'text,referenced_tweets,entities',\n",
    "                      'expansions': 'referenced_tweets.id'\n",
    "                      }\n",
    "    resp = requests.get(\n",
    "        twitter_url, headers=twitter_heeaders, params=twitter_params)\n",
    "    resp_json = resp.json()\n",
    "\n",
    "    for tweet in resp_json['data']:\n",
    "        rt = [rt for rt in resp_json['includes']['tweets']\n",
    "              if rt['id'] == tweet['referenced_tweets'][0]['id']][0]\n",
    "        text = replace_urls(rt)\n",
    "        text = f\"{tweet['text'].split(':')[0]}: {text}\\nhttps://twitter.com/{tweet['text'].split(':')[0].split('@')[-1]}/status/{rt['id']}\"\n",
    "        text = replace_usernames(text)\n",
    "        rt_texts[tweet['id']] = text\n",
    "\n",
    "\n",
    "def add_full_RT_texts(tweets):\n",
    "    # looks for Tweets with media and asks Twitter API for alt texts\n",
    "    # adds those alt texts to the dict alt_texts\n",
    "    truncated_retweets = retweets = [tweet for tweet in tweets if tweet['full_text'].startswith(\n",
    "        'RT @') and tweet['full_text'].endswith('â€¦')]\n",
    "    print(f'Found {len(truncated_retweets)} truncated Retweets.')\n",
    "    tweet_ids = [str(tweet['id']) for tweet in truncated_retweets]\n",
    "    batches = [tweet_ids[idx:idx+100] for idx in range(0, len(tweet_ids), 100)]\n",
    "\n",
    "    for batch in tqdm(batches):\n",
    "        retrieve_rt_texts(batch)\n",
    "\n",
    "    print(f'Collected {len(rt_texts)} full texts for Retweets.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = load_tweets()\n",
    "len(tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_texts = {}\n",
    "add_alt_texts(tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_texts = {}\n",
    "add_full_RT_texts(tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_dict = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-25T08:28:46.728201Z",
     "start_time": "2022-12-25T08:28:46.566070Z"
    }
   },
   "outputs": [],
   "source": [
    "for tweet in tqdm(tweets[25218+4828:]):\n",
    "\n",
    "    if tweet['id'] in ids_dict:\n",
    "        # was already posted, we can skip it\n",
    "        pass\n",
    "    elif tweet['full_text'].startswith('RT @'):\n",
    "        # Retweets are often truncated and full data needs to be retreived from the API\n",
    "        if rt_texts.get(tweet['id']):\n",
    "            toot = {'status': rt_texts.get(tweet['id']),\n",
    "                    'visibility': 'public',\n",
    "                    'created_at': to_timestamp(tweet['created_at']),\n",
    "                    'language': tweet['lang']\n",
    "                    }\n",
    "        else:\n",
    "            toot = tweet_to_toot(tweet)\n",
    "        posted = post_status(toot)\n",
    "        ids_dict[tweet['id']] = posted['id']\n",
    "    else:\n",
    "        toot = tweet_to_toot(tweet)\n",
    "        if 'media' in tweet['entities']:\n",
    "            # upload media to append to the post\n",
    "            media_ids = []\n",
    "            for media in tweet['entities']['media']:\n",
    "                image_path = f\"{media_dir}{tweet['id']}-{media['media_url_https'].split('/')[-1]}\"\n",
    "                if not Path(image_path).is_file():\n",
    "                    image_path = f\"{media_dir_backup}{tweet['id']}-{media['media_url_https'].split('/')[-1]}\"\n",
    "                    if not Path(image_path).is_file():\n",
    "                        continue\n",
    "                file = open(image_path, 'rb')\n",
    "                data = file.read()\n",
    "                url = f\"{API_BASE_URL}/api/v2/media\"\n",
    "                files = {\n",
    "                    'file': (image_path, data, 'application/octet-stream')}\n",
    "                if alt_texts.get('3_' + media['id']):\n",
    "                    values = {'description': alt_texts.get('3_' + media['id'])}\n",
    "                    r = requests.post(url, files=files,\n",
    "                                      data=values, headers=HEADERS)\n",
    "                else:\n",
    "                    r = requests.post(url, files=files, headers=HEADERS)\n",
    "                json_data = r.json()\n",
    "                media_ids.append(json_data['id'])\n",
    "                toot['status'] = toot['status'].replace(media['url'], '')\n",
    "            toot['media_ids[]'] = media_ids\n",
    "        if 'in_reply_to_screen_name' in tweet and tweet['in_reply_to_screen_name'] == 'luca':\n",
    "            # if Tweet is part of a thread, get ID if previous post\n",
    "            try:\n",
    "                toot['in_reply_to_id'] = ids_dict.get(\n",
    "                    tweet['in_reply_to_status_id'])\n",
    "            except:\n",
    "                print(tweet)\n",
    "        # print(tweet)\n",
    "        # print(toot)\n",
    "        posted = post_status(toot)\n",
    "        # print(posted)\n",
    "        ids_dict[tweet['id']] = posted['id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ids_dict.txt', 'w') as f:\n",
    "    f.write(json.dumps(ids_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('alt_texts.txt', 'w') as f:\n",
    "    f.write(json.dumps(alt_texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rt_texts.txt', 'w') as f:\n",
    "    f.write(json.dumps(rt_texts))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todos\n",
    "\n",
    "- [ ] change ids and alt_texts storage from dict to file\n",
    "- [ ] check alt texts (first ones are broken and need to be replaced)\n",
    "- [ ] check images (first ones weren't attached to posts and need to be replaced)\n",
    "- [x] add Retweets (get full text from API and post them; haven't been posted yet)\n",
    "- [ ] fix videos (they weren't uploaded, but linked. :( )\n",
    "- [ ] replace self-quotes with self-posts (update url in posts)\n",
    "- [ ] import mastodon data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 (main, Oct 25 2022, 13:57:33) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
